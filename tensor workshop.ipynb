{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тензоры\n",
    "Тензором размерности $d$ (или, чаще говорят, порядка $d$) называется $d$-мерная таблица ($d$-мерная матрица), составленная из элементов $a(i_1, ..., i_d)$. Иногда тензор размерности $d$ называется также $d$-тензором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_tensor(*shape):\n",
    "    return np.random.rand(*shape)\n",
    "\n",
    "def ones_tensor(*shape):\n",
    "    return np.ones(shape)\n",
    "\n",
    "def zeros_tensor(*shape):\n",
    "    return np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5488135  0.71518937]\n",
      "  [0.60276338 0.54488318]]\n",
      "\n",
      " [[0.4236548  0.64589411]\n",
      "  [0.43758721 0.891773  ]]]\n",
      "[[[1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(rand_tensor(2,2,2))\n",
    "print(ones_tensor(1,2,3))\n",
    "print(zeros_tensor(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матрицы, ассоциированные с тензором\n",
    "\n",
    "Для трехмерного тензора $a(i, j, k)$ размера $n_1 \\times n_2 \\times n_3$ естественным образом определяются матрицы развертки $a_1$, $a_2$ и $a_3$ по каждому из трех измерений:\n",
    "$a_1[i; jk] = a_2[j; ik] = a_3[k; ij] = a[i, j, k]$.\n",
    "\n",
    "Матрицы $a_1$, $a_2$ и $a_3$ имеют соответственно размеры $n_1 \\times n_2 n_3$, $n_2 \\times n_1 n_3$ и $n_3 \\times n_1 n_2$.\n",
    "Точка с запятой используется, чтобы отделить строчные и столбцовые индексы\n",
    "матрицы. $jk$, $ik$ и $ij$ - это составные индексы.\n",
    "\n",
    "По определению, составной индекс $ij$ принимает столько значений, сколько имеется пар значений для $i$ и $j$. При изображении матриц в виде таблиц считается, что для составных индексов используется лексикографический порядок: в паре индексов $(i, j)$ сначала изменяется $j$, затем $i$. Если $i$ и $j$ принимают значения $1, 2, 3$ и\n",
    "$1, 2$ соответственно, то пары $(i, j)$ выстраиваются в последовательность $$(1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2).$$\n",
    "Аналогичным образом определяется лексикографический порядок для составных индексов $ijk$, $ijkl$ и так далее.\n",
    "\n",
    "Матрицы $a(i_1 ... i_k; i_{k+1} ... i_d)$ называются стандартными ассоциированными матрицами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold_tensor(tensor, rows_indexes_number=0):\n",
    "    n = rows_indexes_number\n",
    "    rows = int(np.product(tensor.shape[:n]))\n",
    "    cols = int(np.product(tensor.shape[n:]))\n",
    "    return np.reshape(tensor, (rows, cols))\n",
    "\n",
    "def fold_matrix(matrix, shape):\n",
    "    return np.reshape(matrix, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1111 1112]\n",
      "   [1121 1122]]\n",
      "\n",
      "  [[1211 1212]\n",
      "   [1221 1222]]]\n",
      "\n",
      "\n",
      " [[[2111 2112]\n",
      "   [2121 2122]]\n",
      "\n",
      "  [[2211 2212]\n",
      "   [2221 2222]]]]\n"
     ]
    }
   ],
   "source": [
    "t = np.empty((2,2,2,2), dtype=int)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                t[i][j][k][l] = int(f'{i + 1}{j + 1}{k + 1}{l + 1}')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1111 1112 1121 1122 1211 1212 1221 1222 2111 2112 2121 2122 2211 2212\n",
      "  2221 2222]]\n"
     ]
    }
   ],
   "source": [
    "m0 = unfold_tensor(t)\n",
    "print(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1111 1112 1121 1122 1211 1212 1221 1222]\n",
      " [2111 2112 2121 2122 2211 2212 2221 2222]]\n"
     ]
    }
   ],
   "source": [
    "m1 = unfold_tensor(t, 1)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1111 1112 1121 1122]\n",
      " [1211 1212 1221 1222]\n",
      " [2111 2112 2121 2122]\n",
      " [2211 2212 2221 2222]]\n"
     ]
    }
   ],
   "source": [
    "m2 = unfold_tensor(t, 2)\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1111 1112]\n",
      " [1121 1122]\n",
      " [1211 1212]\n",
      " [1221 1222]\n",
      " [2111 2112]\n",
      " [2121 2122]\n",
      " [2211 2212]\n",
      " [2221 2222]]\n"
     ]
    }
   ],
   "source": [
    "m3 = unfold_tensor(t, 3)\n",
    "print(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(fold_matrix(m0, t.shape), t)\n",
    "assert np.array_equal(fold_matrix(m1, t.shape), t)\n",
    "assert np.array_equal(fold_matrix(m2, t.shape), t)\n",
    "assert np.array_equal(fold_matrix(m3, t.shape), t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Норма Фробениуса одна и та же для любой матрицы, ассоциированной с одним и тем же тензором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fro(matrix):\n",
    "    return la.norm(matrix, 'fro')\n",
    "\n",
    "def dist(m1, m2):\n",
    "    return fro(m1 - m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert fro(m0) == fro(m1) == fro(m2) == fro(m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ранговые матричные разложения\n",
    "\n",
    "Пусть $A$ - произвольная матрица размера $m\\times n$ ранга $r$, тогда ранговое разложение имеет вид $A = UV$, где $U$ — матрица $m\\times r$ и $V$ — матрица $r\\times n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скелетное разложение\n",
    "Матрица $A = a(i, j)$ с элементами вида $a[i, j] = u(i)v(j)$ называется скелетоном.\n",
    "Представление матрицы в виде суммы скелетонов называется ее скелетным разложением или каноническим разложением. Сумма $r$ скелетонов определяется системами столбцов $u_1, ..., u_r$ и $v_1, ..., v_r$ и имеет вид $$A = \\sum_{\\alpha=1}^r u_\\alpha v_\\alpha^T = UV,$$\n",
    "где $$U = [u_1, ..., u_r], V = [v_1, ..., v_r]^T$$\n",
    "Минимально возможное число скелетонов называется рангом матрицы $a(i, j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_decomposition(matrix):\n",
    "    raise NotImplementedError('DO IT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сингулярное разложение (SVD)\n",
    "\n",
    "Пусть имеется скелетное разложение комплексной матрицы A, записанное в виде $$A = \\sum_{\\alpha = 1}^r = \\sigma_\\alpha u_\\alpha v_\\alpha^*,$$\n",
    "где $$\\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_r \\ge 0$$\n",
    "и каждая из систем столбцов $u_1, ..., u_r$ и $v_1, ..., v_r$ является ортонормированной при естественном скалярном произведении. Такое разложение называется сингулярным разложением матрицы $A$.\n",
    "\n",
    "Пусть $A_k$ получается из сингулярного разложения матрицы $A$ отбрасыванием скелетонов с номерами $\\alpha \\ge k + 1$. Тогда\n",
    "$$\\min_{rankB \\le k}||A - B||_2 = ||A - A_k||_2 = \\sigma_{k+1},$$\n",
    "$$\\min_{rankB \\le k}||A - B||_F = ||A - A_k||_F = \\sqrt{\\sum_{\\alpha \\ge k + 1}\\sigma_\\alpha^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _svd(matrix, eps=1e-6):\n",
    "    u, s, v = la.svd(matrix, full_matrices=False)\n",
    "    id_ = s.size\n",
    "    for i, s_val in enumerate(s):\n",
    "        if s_val <= eps:\n",
    "            id_ = i\n",
    "            break\n",
    "    id_ = max(id_, 1)\n",
    "    return u[:,:id_], s[:id_], v[:id_,:]\n",
    "\n",
    "def svd_decomposition(matrix, eps=1e-6):\n",
    "    u, s, v = _svd(matrix, eps)\n",
    "    return u, np.diag(s) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(3,)\n",
      "(3, 4)\n",
      "[5.17842823 2.58487444 0.70873494]\n"
     ]
    }
   ],
   "source": [
    "m = [\n",
    "    [1, 0, 2, 0],\n",
    "    [0, 1, 0, 3],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 2, 3],\n",
    "]\n",
    "\n",
    "u, s, v = _svd(m)\n",
    "for i in (u, s, v):\n",
    "    print(i.shape)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9825620866736277e-15\n"
     ]
    }
   ],
   "source": [
    "u, v = svd_decomposition(m)\n",
    "print(dist(m, u @ v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(2, 4)\n",
      "0.7087349353283674\n",
      "0.7087349353283673\n"
     ]
    }
   ],
   "source": [
    "u, v = svd_decomposition(m, eps=s[-1])\n",
    "for i in (u, v):\n",
    "    print(i.shape)\n",
    "print(dist(m, u @ v))\n",
    "print(s[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(1, 4)\n",
      "2.680276309338857\n",
      "2.680276309338856\n"
     ]
    }
   ],
   "source": [
    "u, v = svd_decomposition(m, eps=s[-2])\n",
    "for i in (u, v):\n",
    "    print(i.shape)\n",
    "print(dist(m, u @ v))\n",
    "print((s[-1]**2 + s[-2]**2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разложение в тензорный поезд\n",
    "\n",
    "$$T(n_1, n_2, ..., n_d) = G(n_1, r_1) G(r_1, n_2, r_2) G(r_2, n_3, r_3) ... G(r_{d-2}, n_{d-1}, r_{d-1}) G(r_{d-1}, n_d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/tt_tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt_cores(tensor, mtrx_decomposition_func=svd_decomposition):\n",
    "    \"\"\"\n",
    "    Decompose tensor of size (n1, n2, ..., nd) to\n",
    "    list of TT cores of sizes\n",
    "    [(n1, r1), (r1, n2, r2), (r2, n2, r3), ..., (rd, nd)]\n",
    "    \"\"\"\n",
    "    decomposition = []\n",
    "    \n",
    "    t = tensor\n",
    "    while True:\n",
    "        mtrx = unfold_tensor(t, 1)\n",
    "        u, v = mtrx_decomposition_func(mtrx)\n",
    "        decomposition.append(u)\n",
    "        \n",
    "        if t.ndim == 2:\n",
    "            decomposition.append(v)\n",
    "            break\n",
    "        \n",
    "        # Get next step tensor of size (r_i * n_{i+1}, n_{i+2}, ...)\n",
    "        r = v.shape[0]\n",
    "        n = t.shape[1]\n",
    "        t = fold_matrix(v, (r * n, *t.shape[2:]))\n",
    "    \n",
    "    for i in range(1, len(decomposition) - 1):\n",
    "        # Transform (r_{i-1} * n_i, r_i) matrix\n",
    "        # to (r_{i-1}, n_i, r_i) tensor\n",
    "        prev_r = decomposition[i - 1].shape[-1]\n",
    "        r = decomposition[i].shape[1]\n",
    "        n = decomposition[i].shape[0] // prev_r\n",
    "        u = fold_matrix(decomposition[i], (prev_r, n, r))\n",
    "        decomposition[i] = u\n",
    "\n",
    "    return decomposition\n",
    "\n",
    "\n",
    "def from_tt_cores(cores):\n",
    "    cores_iterator = iter(cores)\n",
    "    tensor = next(cores_iterator)\n",
    "    for core in cores_iterator:\n",
    "        # Dot product over rank indexes\n",
    "        # of tensor of size (n_1, ..., n_{i-1}, r_{i-1})\n",
    "        # and core of size (r_{i-1}, n_i, r_i)\n",
    "        # Result is a tensor of size (n_1, ..., n_{i-1}, n_i, r_i)\n",
    "        tensor = np.tensordot(tensor, core, axes=([-1], [0]))\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2, 4)\n",
      "(4, 2, 4)\n",
      "(4, 2, 2)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "t = rand_tensor(2,2,2,2,2)\n",
    "cores = tt_cores(t)\n",
    "for c in cores:\n",
    "    print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 2, 2)\n",
      "(2, 2, 2, 2, 2)\n",
      "3.0123751623875368e-15\n"
     ]
    }
   ],
   "source": [
    "restored_t = from_tt_cores(cores)\n",
    "print(t.shape)\n",
    "print(restored_t.shape)\n",
    "print(dist(unfold_tensor(t), unfold_tensor(restored_t)))\n",
    "# Check tensors are element-wise equal within a tolerance\n",
    "assert np.allclose(t, restored_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tensor with 32768 elements reduced to 30 elements\n",
      "2. Tensor with 32768 elements reduced to 170 elements\n",
      "3. Tensor with 32768 elements reduced to 76456 elements\n"
     ]
    }
   ],
   "source": [
    "t = ones_tensor(*(2,) * 15)\n",
    "cores = tt_cores(t)\n",
    "assert np.allclose(t, from_tt_cores(cores))\n",
    "print('1. Tensor with {} elements reduced to {} elements'.format(t.size, sum(c.size for c in cores)))\n",
    "\n",
    "t = np.array([math.sin(x) / x for x in np.arange(1.0, 2.0, 1 / (2**15))])\n",
    "t = t.reshape((2,) * 15)\n",
    "cores = tt_cores(t)\n",
    "assert np.allclose(t, from_tt_cores(cores))\n",
    "print('2. Tensor with {} elements reduced to {} elements'.format(t.size, sum(c.size for c in cores)))\n",
    "\n",
    "t = rand_tensor(*(2,) * 15)\n",
    "cores = tt_cores(t)\n",
    "assert np.allclose(t, from_tt_cores(cores))\n",
    "print('3. Tensor with {} elements reduced to {} elements'.format(t.size, sum(c.size for c in cores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Операции над тензорами в ТТ-формате"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/tt_operations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Умножение на число\n",
    "\n",
    "Пусть $B = A \\cdot k$, в индексной форме: $$B(i_1, i_2, ..., i_d) = A(i_1, i_2, ..., i_d) \\cdot k.$$\n",
    "Тогда для получения ТТ-представления тензора $B$ надо умножить любое из TT-ядер $A_k$ на $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tt_by_number(cores, k):\n",
    "    \"\"\"\n",
    "    Return TT cores of tensor in TT format multiplied by k,\n",
    "    probably you should multiply random core to prevent precision errors\n",
    "    \"\"\"\n",
    "    new_cores = np.copy(cores)\n",
    "    new_cores[0] = new_cores[0] * k\n",
    "    return new_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = rand_tensor(2,3,4)\n",
    "k = 5\n",
    "cores = tt_cores(t)\n",
    "tk = from_tt_cores(mult_tt_by_number(cores, k))\n",
    "assert np.allclose(t * k, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сумма тензоров\n",
    "\n",
    "Пусть $C = A + B$, в индексной форме: $$C(i_1, i_2, ..., i_d) = A(i_1, i_2, ..., i_d) + B(i_1, i_2, ..., i_d).$$\n",
    "Тогда ТТ-ядра $C_k$ тензора $C$ находятся как\n",
    "$$C_k[i_k] = \\begin{bmatrix}A_k[i_k] && 0 \\\\ 0 && B_k[i_k]\\end{bmatrix}, k = 2, ..., d - 1,$$\n",
    "$$C_1[i_1] = \\begin{bmatrix}A_1[i_1] && B_1[i_1]\\end{bmatrix}, C_d[i_d] = \\begin{bmatrix}A_d[i_d] \\\\ B_d[i_d]\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _union_cores(a, b):\n",
    "    \"\"\"\n",
    "    Create from cores (a1, n, a2) and (b1, n, b2) new core (a1 + b1, n, a2 + b2)\n",
    "    Сore a in upper left corner, b in bottom right. Other elements are zeroes\n",
    "    \"\"\"\n",
    "    part1 = np.concatenate((a, np.zeros(a.shape[:2] + (b.shape[2],))), axis=2)\n",
    "    part2 = np.concatenate((np.zeros(b.shape[:2] + (a.shape[2],)), b), axis=2)\n",
    "    return np.concatenate((part1, part2))\n",
    "\n",
    "\n",
    "def sum_tt(a, b):\n",
    "    \"\"\"\n",
    "    Sum tensores in TT format.\n",
    "    Return cores of (a + b) tensor\n",
    "    TODO:round this!\n",
    "    \"\"\"\n",
    "    new_cores = []\n",
    "    for i in range(len(a)):\n",
    "        if i == 0:\n",
    "            new_cores.append(np.concatenate((a[i], b[i]), axis=1))\n",
    "        elif i == len(a) - 1:\n",
    "            new_cores.append(np.concatenate((a[i], b[i])))\n",
    "        else:\n",
    "            new_cores.append(_union_cores(a[i], b[i]))\n",
    "    return np.array(new_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = ones_tensor(2,3,2)\n",
    "t1 = rand_tensor(2,3,2)\n",
    "cores_0 = tt_cores(t0)\n",
    "cores_1 = tt_cores(t1)\n",
    "t2 = from_tt_cores(sum_tt(cores_0, cores_1))\n",
    "\n",
    "assert np.allclose(t2, t0 + t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэлементное умножение\n",
    "\n",
    "Пусть $C = A \\odot B$, в индексной форме: $$C(i_1, i_2, ..., i_d) = A(i_1, i_2, ..., i_d) \\cdot B(i_1, i_2, ..., i_d).$$\n",
    "Тогда ТТ-ядра $C_k$ тензора $C$ могут быть получены как $$C_k[i_k] = A_k[i_k] \\otimes B_k[i_k],$$\n",
    "где $\\otimes$ - произведение Кронекера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сумма элементов тензора\n",
    "\n",
    "Элемент тензора $t_d$ представим в ТТ-формате как $$t(i_1, i_2, ..., i_d) = G_1[i_1] G_2[i_2] ... G_d[i_d]$$\n",
    "Тогда, $$Sum(t_d) = \\sum_{i_1,i_2,...,i_d}t(i_1, i_2, ..., i_d) = \\sum_{i_1,i_2,...,i_d}G_1[i_1] G_2[i_2] ... G_d[i_d] = \\left(\\sum_{i_1}G_1[i_1]\\right)_{1 \\times r_1} \\left(\\sum_{i_2}G_2[i_2]\\right)_{r_1 \\times r_2} ... \\left(\\sum_{i_d}G_d[i_d]\\right)_{r_{d-1} \\times 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_tensor_elements(cores):\n",
    "    \"\"\"\n",
    "    Returns sum of tensor elements by TT format\n",
    "    \"\"\"\n",
    "    res = np.sum(cores[0], axis=0)\n",
    "    for c in cores[1:]:\n",
    "        val = np.sum(c, axis=1)\n",
    "        res = res @ val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = rand_tensor(3, 5, 2, 5)\n",
    "cores = tt_cores(t)\n",
    "assert abs(sum_of_tensor_elements(cores) - np.sum(t)) < 1e-12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
